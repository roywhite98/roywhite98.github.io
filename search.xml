<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>为什么向量化计算？</title>
    <url>/posts/8b99bfe9.html</url>
    <content><![CDATA[<p>向量化计算可以避免显式的for循环，<br>这样可以更好发挥CPU或者GPU的并行计算能力，从而极大提高计算效率。<br>所以线性代数是个好东西，本科是人都得学真是高瞻远瞩。</p>
]]></content>
  </entry>
  <entry>
    <title>小无相功：pytorch</title>
    <url>/posts/89838bc0.html</url>
    <content><![CDATA[<p>看官网呗还等着我写推教学吗？</p>
]]></content>
  </entry>
  <entry>
    <title>CV吸星大法（一）：卷积神经网络CNN</title>
    <url>/posts/9cab57d6.html</url>
    <content><![CDATA[<h2 id="先整个原理！惯例，先上名词解释"><a href="#先整个原理！惯例，先上名词解释" class="headerlink" title="先整个原理！惯例，先上名词解释"></a>先整个原理！惯例，先上名词解释</h2><p><strong>人工神经网络（Aritificial Neural Network, ANN）</strong><br>简称神经网络（Neural Network, NN），简称神经网络。在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗地讲就是具备学习功能。<br>现代神经网络是一种非线性统计性数据建模工具，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。</p>
<p>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p>
<p>在人工神经网络中，简单的人工节点，称作神经元（neurons）</p>
<p>人工神经网络目前没有一个统一的正式定义。不过，具有下列特点的统计模型可以被称作是“神经化”的：</p>
<ul>
<li>具有一组可以被调节的权重（被学习算法调节的数值参数）</li>
<li>可以估计输入数据的非线性函数关系</li>
</ul>
<p>这些可调节的权重可以被看做神经元之间的连接强度。</p>
<p>人工神经网络与生物神经网络的相似之处在于，它可以集体地、并行地计算函数的各个部分，而不需要描述每一个单元的特定任务。神经网络这个词一般指统计学、认知心理学和人工智能领域使用的模型，而控制中央神经系统的神经网络属于理论神经科学和计算神经科学。</p>
<p>典型的人工神经网络具有以下三个部分：</p>
<p><strong>结构（Architecture）</strong> 结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。</p>
<p><strong>激励函数（Activation Rule）</strong> 大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。  </p>
<p><strong>学习规则（Learning Rule）</strong> 学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激发。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激发。最后，输出神经元的激励值决定了识别出来的是哪个字母。</p>
<p>单个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。</p>
<p>单个神经元的作用：把一个n维向量空间用一个超平面分割成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。</p>
<p>该超平面的方程：<br>$$ \vec{W’}\vec p + b &#x3D; 0$$</p>
<ul>
<li>$\vec{W’}$ 权向量</li>
<li>$b$ 偏置</li>
<li>$\vec p$ 超平面上的向量</li>
</ul>
<p>人工神经网络分类：</p>
<ol>
<li>依学习策略（Algorithm）分类主要有：</li>
</ol>
<ul>
<li>监督式学习网络（Supervised Learning Network）为主<br>  无监督式学习网络（Unsupervised Learning Network）</li>
<li>混合式学习网络（Hybrid Learning Network）</li>
<li>联想式学习网络（Associate Learning Network）<br>最适化学习网络（Optimization Application Network）</li>
</ul>
<ol start="2">
<li>依网络架构（Connectionism）分类主要有：</li>
</ol>
<ul>
<li>前馈神经网络（Feed Forward Network）</li>
<li>循环神经网络（Recurrent Network）</li>
<li>强化式架构（Reinforcement Network）</li>
</ul>
<p><strong>卷积神经网络（Convolutional Neural Network, CNN）</strong><br>一种前馈神经网络。它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。<br>卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。<br>卷积神经网络的灵感来自于动物视觉皮层组织的神经连接方式。单个神经元只对有限区域内的刺激作出反应，不同神经元的感知区域相互重叠从而覆盖整个视野。</p>
<h3 id="卷积神经网络的结构"><a href="#卷积神经网络的结构" class="headerlink" title="卷积神经网络的结构"></a>卷积神经网络的结构</h3><p>卷积神经网络由输入层、隐藏层和输出层组成。</p>
<p><strong>卷积层</strong><br>卷积层可以产生一组平行的特征图（feature map），它通过在输入图像上滑动不同的卷积核并执行一定的运算而组成。</p>
<p><strong>线性整流层</strong><br>线性整流层（Rectified Linear Units layer, ReLU layer）使用线性整流（Rectified Linear Units, ReLU）<br>$$ f(x)&#x3D;max(0,x)$$<br>作为这一层神经的激励函数（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。<br>事实上，其他的一些函数也可以用于增强网络的非线性特征，如双曲正切函数<br>$$f(x)&#x3D;tanh(x),<br>f(x)&#x3D;|tanh(x)|$$<br>或者Sigmoid函数<br>$$f(x)&#x3D;(1+e^{-x})^{-1}$$<br>相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍，而并不会对模型的泛化准确度造成显著影响。</p>
<p><strong>池化层</strong><br>池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种非线性形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。</p>
<p>直觉上，这种机制能够有效地原因在于，一个特征的精确位置远不及它相对于其他特征的粗略位置重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的网络结构中的卷积层之间都会周期性地插入池化层。池化操作提供了另一种形式的平移不变性。因为卷积核是一种特征发现器，我们通过卷积层可以很容易地发现图像中的各种边缘。但是卷积层发现的特征往往过于精确，我们即使高速连拍拍摄一个物体，照片中的物体的边缘像素位置也不大可能完全一致，通过池化层我们可以降低卷积层对边缘的敏感性。</p>
<p>池化层每次在一个池化窗口（depth slice）上计算输出，然后根据步幅移动池化窗口。下图是目前最常用的池化层，步幅为2，池化窗口为2×2的二维最大池化层。每隔2个元素从图像划分出2×2的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png" alt="Alt text"></p>
<p>$$f_{X,Y}(S)&#x3D;\mathop{max}\limits^1_{a,b&#x3D;0}S_{2X+a, 2Y+b}$$</p>
<p>除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“L2-范数池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。</p>
<p>由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，[4]甚至不再使用池化层。[5]</p>
<p>RoI池化(Region of Interest)是最大池化的变体，其中输出大小是固定的，输入矩形是一个参数。[6]</p>
<p>池化层是基于 Fast-RCNN [7]架构的卷积神经网络的一个重要组成部分。</p>
<p><strong>完全连接层</strong><br>最后，在经过几个卷积和最大池化层之后，神经网络中的高级推理通过完全连接层来完成。就和常规的非卷积人工神经网络中一样，完全连接层中的神经元与前一层中的所有激活都有联系。因此，它们的激活可以作为仿射变换来计算，也就是先乘以一个矩阵然后加上一个偏差(bias)偏移量(向量加上一个固定的或者学习来的偏差量)。</p>
<p><strong>微调（fine-tuning）</strong><br>卷积神经网络（例如Alexnet、VGG网络）在网络的最后通常为softmax分类器。微调一般用来调整softmax分类器的分类数。例如原网络可以分类出2种图像，需要增加1个新的分类从而使网络可以分类出3种图像。微调（fine-tuning）可以留用之前训练的大多数参数，从而达到快速训练收敛的效果。例如保留各个卷积层，只重构卷积层后的全连接层与softmax层即可。</p>
<h2 id="好了朋友-你为什么要用卷积神经网络呢"><a href="#好了朋友-你为什么要用卷积神经网络呢" class="headerlink" title="好了朋友 你为什么要用卷积神经网络呢"></a>好了朋友 你为什么要用卷积神经网络呢</h2><p>在 CNN 出现之前，图像对于人工智能来说是一个难题，有2个原因：</p>
<ol>
<li>图像需要处理的数据量太大，导致成本很高，效率很低</li>
<li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</li>
</ol>
<p>卷积神经网络 – CNN 解决的第一个问题就是“将复杂问题简化”，把大量参数降维成少量参数，再做处理。</p>
<p>更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。</p>
<p><img src="https://easyai.tech/wp-content/uploads/2022/08/98412-2019-06-12-tuxiangtx.png.webp" alt="Alt text"></p>
<p>假如有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，图像的内容（本质）并没有发生变化，只是位置发生了变化。</p>
<p>CNN 用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。</p>
<p>典型的 CNN 由3个部分构成：</p>
<ul>
<li>卷积层（提取特征）</li>
<li>池化层（下采样，数据降维）</li>
<li>全连接层（输出结果）</li>
</ul>
<p>如果简单来描述的话：</p>
<p>卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。</p>
<p>其实有四大要件：</p>
<ul>
<li>卷积层（Conv Layer）</li>
<li>非线性激励层（Non-linear Layer）</li>
<li>池化层（Pooling Layer）</li>
<li>输出层（Fully-connected Layer）</li>
</ul>
<p><strong>过拟合（overfitting）</strong><br><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1024px-Overfitting.svg.png" alt="Alt text"></p>
<pre><code>绿线代表过适模型，黑线代表正则化模型。虽然绿线完美的符合训练数据，但调适得太过紧密或精确；并且与黑线相比，在新的测试资料上会有更高的错误率。
</code></pre>
<p>过拟合模型指的是相较有限的数据而言，参数过多或者结构过于复杂的统计模型。<br>过拟合最显著的后果就是在验证集上的效果很差。  </p>
<p><strong>反向传播算法</strong><br>Backpropagation，意为误差反向传播，缩写为BP，是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变量计算损失函数的梯度，以更新权重来最小化损失函数。<br>反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。</p>
<p>第1阶段：激励传播<br>每次迭代中的传播环节包含两步：</p>
<p>（前向传播阶段）将训练输入送入网络以获得预测结果；<br>（反向传播阶段）对预测结果同训练目标求差(损失函数)。<br>第2阶段：权重更新<br>对于每个突触上的权重，按照以下步骤进行更新：</p>
<p>将输入激励和响应误差相乘，从而获得权重的梯度；<br>将这个梯度乘上一个比例并取反后加到权重上。<br>这个比例（百分比）将会影响到训练过程的速度和效果，因此成为“训练因子”。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。</p>
<p>第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。</p>
]]></content>
  </entry>
  <entry>
    <title>conda env下没有python</title>
    <url>/posts/7e7d79c3.html</url>
    <content><![CDATA[<p>一般用conda新建一个环境是这样的：<br><code>conda create -n your_env</code><br>那这个就不包含解释器<br>得这么整<br><code>conda create -n your_env python=3.9</code><br>那已经安装了一个空的咋办：<br><code>conda install -n your_env python</code></p>
<p>ps: <code>-n</code> 是 <code>--name</code>的缩写</p>
]]></content>
  </entry>
  <entry>
    <title>CV吸星大法（二）：卷积神经网络CNN</title>
    <url>/posts/51dcf902.html</url>
    <content><![CDATA[<h2 id="先去看书！先去看书！先去看书！"><a href="#先去看书！先去看书！先去看书！" class="headerlink" title="先去看书！先去看书！先去看书！"></a>先去看书！先去看书！先去看书！</h2><p><a href="https://github.com/exacity/deeplearningbook-chinese">Deeplearningbook中文版</a><br><a href="https://github.com/ionvision/DeepLearningCourseCodes">github上的相关课程</a></p>
<h2 id="急功近利，先看U-Net"><a href="#急功近利，先看U-Net" class="headerlink" title="急功近利，先看U-Net"></a>急功近利，先看U-Net</h2><p>虽然讲说基础不牢地动山摇，但人生苦短。<br>反正我也是来做图像分割的，那先从图像分割倒序学习也没什么问题吧。</p>
<p><a href="https://arxiv.org/pdf/1505.04597v1.pdf">遇事不决先看U-Net原文</a></p>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>激励函数<br><strong>激活函数是用来加入非线性因素的，因为线性模型的表达力不够。</strong><br>假设如果没有激活函数的出现，你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，也就是说没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。就是最原始的感知机（Perceptron），网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可逼近任意函数）<br>二分类问题有无激活函数的差异：<br><img src="/posts/51dcf902.htm/14512145-36dd22b3f08051a3.webp" alt="Alt text"><br><img src="/posts/51dcf902.htm/14512145-cb0945e166ab4608.webp" alt="Alt text"></p>
<p>神经网络的数学基础是处处可微的，选取的激活函数要能保证数据输入与输出也是可微的，运算特征是不断进行循环计算，所以在每代循环过程中，每个神经元的值也是在不断变化的。</p>
<p>ReLU函数</p>
<p>$$ f(x)&#x3D;max(0,x)$$</p>
<p><strong>ReLU其实就是个取最大值的函数。</strong><br>ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。（也就是说：在输入是负值的情况下，它会输出0，那么神经元就不会被激活。这意味着同一时间只有部分神经元会被激活，从而使得网络很稀疏，进而对计算来说是非常有效率的。）正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。</p>
<p>选用ReLU函数作为激活函数的好处有以下几个：</p>
<p>①没有饱和区，不存在梯度消失问题。</p>
<p>②没有复杂的指数运算，计算简单、效率提高。</p>
<p>③实际收敛速度较快，比 Sigmoid&#x2F;tanh 等激活函数快很多。</p>
<p>④比 Sigmoid 更符合生物学神经激活机制。</p>
<p>当然relu也存在不足：就是训练的时候很”脆弱”，很容易就die了. 举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了，那么这个神经元的梯度就永远都会是0.实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。</p>
<p><strong>Leaky ReLU函数</strong></p>
<p>$$f(x)&#x3D;max(0.01x,x)$$</p>
<p><img src="/posts/51dcf902.htm/def23ee5b79e408e9ef8098a3ecec476.jpg" alt="Alt text"></p>
<p>可以回避一些ReLU的问题，具有ReLU所有有点，不会有Dead ReLU的问题。</p>
<p><strong>ELU (Exponential Linear Units) 函数</strong></p>
<p>$$ f(x)&#x3D;\left {<br>    \begin{array}{lr}<br>    x,&amp;\text {if } x&gt;0 \<br>    \alpha(e^x-1),&amp; \text {otherwise} \<br>    \end{array}<br>    \right. $$<br><img src="/posts/51dcf902.htm/8b11ba1e96384d4f94e13c93046ceacd.jpg" alt="Alt text"><br>类似leaky ReLU, 但计算量稍大。</p>
<p>没事就用ReLU, ReLU不好用就换上面这俩，目前别的都不好用，计算量大。</p>
<h3 id="图像的卷积和反卷积（上采样）"><a href="#图像的卷积和反卷积（上采样）" class="headerlink" title="图像的卷积和反卷积（上采样）"></a>图像的卷积和反卷积（上采样）</h3><p>图像的卷积实际上是利用卷积核来对图像进行特征提取的一个过程。卷积核是一个矩阵，通过设计这个矩阵（如矩阵的大小、矩阵中的数值），就可以把相对应的图像特征提取出来，如图像的边缘特征、纹理特征等等。<br><img src="/posts/51dcf902.htm/convExample.png" alt="Alt text"><br><img src="/posts/51dcf902.htm/2018080411520076.gif" alt="Alt text"></p>
<p>从上图可以看到，原图经过卷积后，结果图像比原图小了一圈，这是因为卷积核本身也有大小，导致原图的边缘信息没法完全卷积到所造成的。为了避免这种现象，在对图像进行卷积时，一般会先对原图进行扩大（一般是在图像周围加一圈0），以保证在卷积后的图像大小和原始图像相同。</p>
<p><img src="/posts/51dcf902.htm/2019010221302420.gif" alt="Alt text">  </p>
<p>在5×5原图（蓝色）的外面扩大一圈（白色）后，通过3×3卷积核卷积的结果图（绿色）能够同样保持5×5大小。</p>
<p>那如果把原图再往外扩大几圈呢？<br><img src="/posts/51dcf902.htm/20190101215139260.gif" alt="Alt text"></p>
<p>把2×2原图扩大了两圈，再通过3×3的卷积核，卷积结果图像被增大为4×4的大小。</p>
<p>由以上所有结果可见，卷积其实一共有3种情况：</p>
<p>①卷积后，结果图像比原图小：称之为valid卷积</p>
<p>②卷积后，结果图像与原图大小相同：称之为same卷积</p>
<p>③卷积后，结果图像比原图大：称之为full卷积</p>
<p>其中，full卷积其实就是反卷积的过程。到这里应该可以意识到，反卷积实际上也是一种特殊的卷积方式，它可以通过full卷积将原图扩大，增大原图的分辨率，所以对图像进行反卷积也称为对图像进行“上采样”。因此，也可以很直接地理解到，图像的卷积和反卷积并不是一个简单的变换、还原过程，也就是先把图片进行卷积，再用同样的卷积核进行反卷积，是不能还原成原图的，因为反卷积后只是单纯地对图片进行扩大处理，并不能还原成原图像。<br><img src="/posts/51dcf902.htm/20200622104403682.png" alt="Alt text"><br>蓝色是3×3的卷积核，在原图进行卷积和反卷积后，最后得到的图像跟原图是不一致的。因此，通过反卷积并不能还原卷积之前的矩阵，只能从大小上进行还原，因为反卷积的本质还是卷积。如果想要还原成原图像，只能通过专门设计不同的卷积核来实现。</p>
<p><strong>池化层（下采样）</strong><br><img src="/posts/51dcf902.htm/20180804142645882.png" alt="Alt text"></p>
<p>池化是一种减少参数的计算方法，而减少参数的方法就是直接把一些删除，池化层就是专门用于降低参数的处理层。池化一般分为最大池化和平均池化，我们更常用的是最大池化。</p>
<p><strong>损失函数</strong><br>Loss Function是用来估量模型的预测值与真实值的不一致程度，它是一个非负实值函数，损失函数越小，模型的训练效果越好，鲁棒性越强。这是神经网络在通过梯度下降等方法调整参数，使得训练结果不断逼近真实值时必须使用到的一个函数。<br><img src="/posts/51dcf902.htm/20200622105753813.png" alt="Alt text"><br>其中：<br><img src="/posts/51dcf902.htm/20200622110006872.png" alt="Alt text"><br>(softmax函数)<br><img src="/posts/51dcf902.htm/20200622110047598.png" alt="Alt text"><br>加权w(x)的原因，和U-net原文献中的分割任务有关，使用U-net对其他图片进行分割时可以不用该权重。</p>
<p>U-Net的结构<br>长成U形。<br><img src="/posts/51dcf902.htm/20200622111042803.png" alt="Alt text"></p>
<p>从最左边开始，输入的是一张572×572×1的图片，然后经过64个3×3的卷积核进行卷积，再通过ReLU函数后得到64个570×570×1的特征通道。然后把这570×570×64的结果再经过64个3×3的卷积核进行卷积，同样通过ReLU函数后得到64个568×568×1的特征提取结果，这就是第一层的处理结果。</p>
<p>第一层的处理结果是568×568×64的特征图片，通过2×2的池化核，对图片下采样为原来大小的一半：284×284×64，然后通过128个卷积核进一步提取图片特征。后面的下采样过程也是以此类推，每一层都会经过两次卷积来提取图像特征；每下采样一层，都会把图片减小一半，卷积核数目增加一倍。最终下采样部分的结果是28×28×1024，也就是一共有1024个特征层，每一层的特征大小为28×28。</p>
<p>右边部分从下往上则是4次上采样过程。从最右下角开始，把28×28×1024的特征矩阵经过512个2×2的卷积核进行反卷积，把矩阵扩大为56×56×512（注意不是1024个卷积核，结果仅仅是右半边蓝色部分的512个特征通道，不包含左边白色的），由于反卷积只能扩大图片而不能还原图片，为了减少数据丢失，采取把左边降采样时的图片裁剪成相同大小后直接拼过来的方法增加特征层（这里才是左半边白色部分的512个特征通道），再进行卷积来提取特征。由于每一次valid卷积都会使得结果变小一圈，因此每次拼接之前都需要先把左边下采样过程中的图片进行裁剪。矩阵进行拼接后，整个新的特征矩阵就变成56×56×1024，然后经过512个卷积核，进行两次卷积后得到52×52×512的特征矩阵，再一次进行上采样，重复上述过程。每一层都会进行两次卷积来提取特征，每上采样一层，都会把图片扩大一倍，卷积核数目减少一半。最后上采样的结果是388×388×64，也就是一共有64个特征层，每一层的特征大小为388×388。</p>
<p>在最后一步中，选择了2个1×1的卷积核把64个特征通道变成2个，也就是最后的388×388×2，其实这里就是一个二分类的操作，把图片分成背景和目标两个类别。</p>
<p><strong>U-net的卷积核大小、卷积核数量、卷积核中的数值、训练深度怎么确定？</strong><br>总而言之，玄学。</p>
<p>①U-net卷积核的大小为什么是3×3？</p>
<p>卷积核的大小可以自己定义，或者根据实验结果的优劣来进行选择。但一般的选择原则为卷积核越小越好，且一般为奇数×奇数的正方形。卷积核越小，能够大大地降低计算复杂性，但其中1×1的卷积核不具有提升感受野的作用，而卷积核为偶数×偶数时不能保证图片在进行same卷积操作后，还能够还原到原本的大小（会比原图多一条边&#x2F;少一条边，自己画个图就明白了），因此通常选用3×3的卷积核。</p>
<p>②每一层的卷积核数量为什么是64→128→256→512→1024？</p>
<p>卷积核的数量也没有标准，也是自己设置或者根据实验结果的好坏进行选择。但是卷积核过少会导致特征提取得不充分，导致训练效果差，而卷积核过多则会大大加大计算量，会存在过多冗余信息。在U-net浅层中，提取的一般是颜色、轮廓等比较浅显的图像特征，因此卷积核数量不需要特别多，而随着U-net训练层数越深，提取的很多都是图片中无法命名的抽象特征，因此需要更多的卷积核才能够把这些特征提取出来。而U-net每一层都会把卷积核数量翻一倍，是因为经过下采样后图片大小会变成原来的一半，因此认为卷积核应该增多一倍才能够更全面地提取图片特征。</p>
<p>③卷积核中的数值如何确定？</p>
<p>前面提到过，卷积核中的数值实际上就相当于是一般全连接神经网络中的权值，在全连接神经网络中，权值的确定一般都是经过“初始化→根据训练结果逐步调整→训练精度达到目标后停止调整→确定权值”这样一个过程，因此U-net卷积核中数值的确定过程也是类似的，一开始也是先用随机数（服从高斯分布）进行初始化，后面则根据前面提到的损失函数逐步对数值进行调整，当训练精度符合要求后停止，即能确定每个卷积核中的数值。而调整卷积核数值的过程，实际上就是U-net的训练过程，当卷积核结束训练确定数值后，则U-net训练完成，下面将可以用测试集来测试网络的鲁棒性和对新的图片进行分割预测。</p>
<p>④U-net训练深度如何确定？</p>
<p>这跟全连接神经网络中“神经网络层数如何确定”这样一个问题是类似的，目前也没有一个专门的标准，一般根据经验选取，或设置多种不同的深度，通过训练效果来选择最优的层数。U-net原文中也没有提到为什么要选择4层，可能是在该训练项目中，4层的分割效果最好。</p>
<p><strong>UNet的创新点</strong><br>overlap-tile策略<br>在U-Net结构中,卷积全程都使用valid来进行卷积,该卷积的特点就是没有padding, 即padding &#x3D; 0,所以特征图(feature map)会越卷越小,它不会越卷越大,导致了最后输出的尺寸回不到原来输入的尺寸</p>
<p>原本以为U-Net是一个绝对对称的结构, 然而并不是,它左右两边feature map的尺寸不一致,原因就是因为valid卷积,只会越卷越小.如下图的层A,如果这个卷积有padding,那么它可以回到32x32,然后再上采样2x2后,层B就变为64x64,依次轮推最终实现结构的绝对对称<br>显然，如果生生把原始图像喂进来，最后输出的结果，会比原始图像小好多。我们希望输入跟输出尺寸一样，即不能强行scale up最后的输出，又不想给每层卷积加padding<br>于是就诞生了重叠-切片(overlap-tile)策略, 该策略的思想是：对图像的某一块像素点（黄框内部分）进行预测时，需要该图像块周围的像素点提供上下文信息（context），以获得更准确的预测。简单地说, 就是在预处理中,对输入图像进行padding, 通过padding扩大输入图像的尺寸，使得最后输出的结果正好是原始图像的尺寸，同时, 输入图像块的边界也获得了上下文信息从而提高预测的精度。<br>在U-Net原文中，用的镜像操作(mirror padding)，实际上可以使用别的padding方法。<br>因此, 使用Overlap-tile策略对数据进行预处理是有必要的,可对任意大的图像进行无缝分割. 而这种方法通常需要将图像进行分块(patch)的时候才使用。</p>
<p>那么为什么要对图像分块(patch)再输入，而不是直接输入整张图像呢？因为内存限制，有的机器内存比较小，需要分块。然后再通过Overlap-tile 策略将分块图像进行预处理扩大输入尺寸。</p>
<p><strong>魔改U-Net</strong></p>
<ol>
<li>改损失函数</li>
<li>改结构，如UNet++<br><a href="https://zhuanlan.zhihu.com/p/44958351">围观UNet大佬</a></li>
</ol>
]]></content>
      <tags>
        <tag>内功</tag>
        <tag>气宗</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode不能正确切换conda环境</title>
    <url>/posts/3a308af5.html</url>
    <content><![CDATA[<p>vscode有个插件叫code runner，非常好看，比控制台直接输出好看多了。<br>控制台嘛就直接conda activate进去就好，虽然我的默认控制台是powershell进不去会报错，但是只要切换称cmd控制台就没什么问题了。<br>code runner死都切换不了，怎么处理呢，很简单，<br>在cmd中先进入目标conda环境，然后用命令启动：<code>code .</code><br>这样就能运行了。</p>
]]></content>
      <tags>
        <tag>工欲善其事</tag>
      </tags>
  </entry>
  <entry>
    <title>Deploy Hexo on another computer</title>
    <url>/posts/2567506c.html</url>
    <content><![CDATA[<p>在另一台电脑上部署一下Hexo。<br>利用git分支来同步hexo。<br>main branch放静态页面文件<br>hexo branch放源码</p>
<ol>
<li>设置hexo branch为default branch</li>
<li>在老电脑上push源文件到hexo branch</li>
<li>在新电脑上clone hexo branch到本地</li>
<li>在新电脑上安装hexo环境</li>
<li>在新电脑上推送源码到hexo branch，生成的静态页面仍推送到main branch，就是这么神奇</li>
<li>使用git add . &amp;&amp; git commit -m “message” &amp;&amp; git push 更新源文件</li>
<li>使用hexo clean &amp;&amp; hexo g &amp;&amp; hexo d推送静态到main</li>
<li>然后报了个错，看了一下主题文件夹是空的，copy一下主题过来，hexo d，完成</li>
<li>每次使用前pull一下以保证同步</li>
</ol>
]]></content>
      <tags>
        <tag>气宗</tag>
        <tag>工欲善其事</tag>
      </tags>
  </entry>
  <entry>
    <title>GWAS龙象般若真经：（其二）流程原理</title>
    <url>/posts/c698f62e.html</url>
    <content><![CDATA[<h2 id="阴极-遗传标记"><a href="#阴极-遗传标记" class="headerlink" title="阴极-遗传标记"></a>阴极-遗传标记</h2><p>遗传标记从何而来？</p>
<ul>
<li><p>重测序：<br>就是有参考基因组的物种，把群体所有个体都测一遍基因组。</p>
</li>
<li><p>转录组：<br>测转录组，可以无参，仅包含基因编码区</p>
</li>
<li><p>简化基因组（RRGS）：<br>限制性内切酶切基因组DNA，并对酶切片段高通量测序</p>
</li>
</ul>
<p><img src="http://www.lc-bio.com/upload/201805/17/201805171448243883.jpg" alt="Alt text"></p>
<ul>
<li>基因芯片</li>
<li>传统分子标记</li>
</ul>
<h3 id="质量控制（QC）"><a href="#质量控制（QC）" class="headerlink" title="质量控制（QC）"></a>质量控制（QC）</h3><p>说得非常高级，其实就是粗暴地删掉一些。<br>比如snp不够多的物种，比如只有少数物种有的snp都要删掉。还有一个动植物常用的MAF过滤，是指的次等位基因频率过滤，次等位是指基因频率比较低的那个等位。就是说有些等位基因在群体中非常罕见，因此就算分析了也没有什么很强的效应，有些甚至就没有差别，分析浪费算力，所以删掉它。</p>
<p>一般plink文件用plink一行解决，不是plink文件那转成plink文件一样一行可以解决。（笑</p>
<p>当然别的文件也是有办法的，我还没做过，暂时不想做。</p>
<p>计算样品杂合度–het<br>F值越小，杂合度越高，F值越大，纯合度越高。<br>计算snp位点杂合度 –hardy<br>Geno，2&#x2F;0&#x2F;6，第一个是次等位基因纯合个数，第二个是杂合个数，第三个是主等位基因纯合个数<br>O(HET),是杂合所在的比值</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>As summary statistic</th>
<th>As inclusion criteria</th>
</tr>
</thead>
<tbody><tr>
<td>个体基因分型缺失率</td>
<td>–missing</td>
<td>–mind N</td>
</tr>
<tr>
<td>SNP基因分型缺失率</td>
<td>–missing</td>
<td>–geno N</td>
</tr>
<tr>
<td>等位基因频率</td>
<td>–freq</td>
<td>–maf N</td>
</tr>
<tr>
<td>哈迪-温伯格平衡</td>
<td>–hardy</td>
<td>–hwe N</td>
</tr>
<tr>
<td>孟德尔误差率</td>
<td>–mendel</td>
<td>–me N M</td>
</tr>
</tbody></table>
<h2 id="阳极-群体材料"><a href="#阳极-群体材料" class="headerlink" title="阳极-群体材料"></a>阳极-群体材料</h2><h3 id="材料种类"><a href="#材料种类" class="headerlink" title="材料种类"></a>材料种类</h3><p>F2群体，自然群体，核心种质</p>
]]></content>
      <categories>
        <category>GWAS</category>
      </categories>
      <tags>
        <tag>内功</tag>
        <tag>气宗</tag>
      </tags>
  </entry>
  <entry>
    <title>GWAS龙象般若真经：（其一）方法原理</title>
    <url>/posts/d0974096.html</url>
    <content><![CDATA[<p>GWAS，Genome wide association study.<br>中文名 全基因组关联分析。<br><a href="#bukan">太长不看版</a></p>
<p><strong>遗传标记（genetic marker）：</strong><br>已知染色体位置的一种序列或基因。在生物个体间有差异因此可用于鉴别个体或物种。其可被描述为可观测的变异。</p>
<p><strong>基因型 genotype：</strong><br>一个生物体内DNA所包含的基因。指的是一个个体所有等位基因的所有基因座上的所有组合。<br><strong>表现型&#x2F;表型 Phenotype：</strong><br>个体实际外表特征。</p>
<p><strong>基因型和表型的关系：</strong><br>基因型对表型有很大影响，但非唯一因素。即使基因型相同的生物也会表现不同表型。这个现象的机理是表观遗传学。相同的基因在不同的生物体中可能有不同的表达。</p>
<p><strong>基因型分型 genotyping：</strong><br>使用生物学检测的方法阐明一个个体基因型的过程。亦称作”基因型检测”genotypic asasy”，常用技术包括PCR,测序,RFLP,AFLP,MLPA。</p>
<p>回到基因型和表型的关系。<br>假定表型是基因型映射的。<br>那么如何确定表型是由什么基因型产生的？<br>提出一种方案，GWAS。</p>
<ol>
<li>测定多个个体基因型</li>
<li>观测可观测性状（表型）</li>
<li>在群体水平统计分析，根据统计量或显著性筛选出可能影响该性状的遗传标记。</li>
</ol>
<p><strong>连锁不平衡 linkage disequilibrium：</strong><br>是指给定种群中不同基因座loci（位点）上的等位基因allele之间的非随机nonrandom关联性。<br>当两个位点的不同等位基因的关联频率高于或低于独立随机关联的条件下的期望频率时，就称两者是连锁不平衡的。<br>连锁不平衡的程度取决于多方面的因素，包括遗传连锁、自然选择、基因重组的概率、突变率、遗传漂变、婚配制度、选型交配以及种群结构。因此，基因组中连锁不平衡的模式是构建它的群体遗传过程的一个强有力的信号。<br>两个相邻的基因A, B，它们的等位基因分别为a, b。后代群体中，实际观察到的单体型基因型AB，度量观察到的单倍型频率与平衡状态下期望频率的偏差：<br>$P(AB)&#x3D;P(A)*P(B)$，则A, B独立遗传；<br>$P(AB)≠P(A)*P(B)$，则A, B存在连锁不平衡。<br>$$ D &#x3D; P(AB)-P(A)P(B) $$</p>
<p><a href="#jump">此处指向哈温平衡</a></p>
<p>考虑简单的两基因座情况，设有A, B两个基因座，每个基因做各有两个等位基因，分别用1,2表示。假设每个单倍体型的频率如下所示：<br><img src="https://gwaslab.files.wordpress.com/2021/04/image-16.png" alt="Alt text"><br>各个等位基因的频率：<br><img src="https://gwaslab.files.wordpress.com/2021/04/image-17.png" alt="Alt text"><br>如果这两个基因座互相独立不相关（也就是连锁平衡 linkage equilibrium 的状态），那么各个单倍型的频率就可以直接算出，为p1q1 ,p1,q2 , p2q1, p2q2</p>
<p>而实际情况中单倍型的频率对于不相关情况下的理论值会产生偏离（deviation），这个偏离原因即为连锁不平衡（ linkage disequilibrium ），偏离的程度通常记为 D （连锁不平衡系数，coefficient of linkage disequilibrium）</p>
<p>$$D&#x3D;x_{11}-p_{1}q_{1}$$  </p>
<p>下图表示了各单倍型频率，各等位基因频率与D之间的关系。</p>
<p><img src="https://gwaslab.files.wordpress.com/2021/04/image-18.png" alt="Alt text">  </p>
<p>但要注意的是，D值并不是一个用来衡量LD的很好的指标，因为D值会受等位基因频率影响，这使得我们无法比较不同频率的等位基因对之间连锁不平衡的大小。<br>Lewontin提出通过标准化D值来解决该问题，即用D值除以理论上D可能的最大绝对值：<br>$$D’&#x3D; \frac{D}{D_{max}}$$<br>其中：<br><img src="https://gwaslab.files.wordpress.com/2021/04/image-62.png" alt="Alt text"><br>这个就是规范化方法，对原始数据线性变换，使结果映射到[0,1]区间。  </p>
<p>但更多的时候我们使用相关系数（correlation coefficient）r2来衡量LD：  </p>
<p><img src="https://gwaslab.files.wordpress.com/2021/04/image-19.png" alt="Alt text">  </p>
<p>协方差&#x2F;俩方差相乘。</p>
<p>$$r^2&#x3D;\frac{(P_{AB}-P_AP_B)^2}{P_AP_aP_BP_b}$$</p>
<p>这个就是简单相关系数，度量两个变量(0,1分布)之间的线性关系，其实还需要开个方。<br>复相关系数：又叫多重相关系数。复相关是指因变量与多个自变量之间的相关关系。例如，某种商品的季节性需求量与其价格水平、职工收入水平等现象之间呈现复相关关系。<br>典型相关系数：是先对原来各组变量进行主成分分析，得到新的线性关系的综合指标，再通过综合指标之间的线性相关系数来研究原各组变量间相关关系。<br>【这里可以改进GWAS噢好像】<br>统计学上：<br>r的性质：</p>
<ol>
<li>$|r| \geq 1$</li>
<li>$|r|&#x3D;1$的充要条件是，存在常数a, b, 使得P{Y&#x3D;a+bX}&#x3D;1</li>
</ol>
<p>由性质衍生：  </p>
<ul>
<li>相关系数定量地刻画了 X 和 Y的相关程度，即r越大，相关程度越大；r&#x3D;0 对应相关程度最低；  </li>
<li>X 和Y 完全相关的含义是在概率为1的意义下存在线性关系，于是r是一个可以表征X 和Y 之间线性关系紧密程度的量。当r较大时，通常说X 和Y相关程度较好；当r较小时，通常说X和Y相关程度较差；当X和Y不相关，通常认为X和Y之间不存在线性关系，<strong>但并不能排除X和Y之间可能存在其他关系</strong>。</li>
</ul>
<p>若X和Y独立，则必有r&#x3D;0，因而X和Y不相关；若X和Y不相关，则仅仅是不存在线性关系，可能存在其他关系，如$X^2+Y^2&#x3D;1$，X和Y不独立。<br>因此，“不相关”是一个比“独立”要弱的概念。</p>
<p>特别地，在GWAS中<br>r2&#x3D;1有更严格的解释：两个位点的等位基因有相同的频率，并且一个位点某个等位基因的出现完全预示着另外一个位点相应等位基因的出现，这时候两个位点组成的四种可能的单倍型仅表现为两种。与D’相比，r2在连锁不平衡中更加有用，因为其具有较强的群体遗传学理论基础和一些统计学上的优势：</p>
<p>r2的期望值和有效种群大小和重组系数相关，r2&#x3D;1&#x2F;(1+4NeC),其中Ne是有小种群大小，C是重组系数。<br>r2有很好的取样特性，样本量和r2的乘积就是所观察到的关联水平概率对应的卡方值。在检测snp和致病位点之间的关联时，如果要达到同样的统计效力，所需要的样本量要增大1&#x2F;r2倍。例如，假设snp1与疾病相关，我们要对它附近的snp2进行基因分型，他们之间的LD系数r2&#x3D;0.5，为了达到与snp1位点检测相同的统计效力，必须把样本增加2倍。<br>与D’相比，在同样长度的染色体范围内，r2往往更低，这个特性能够帮助我们找到更精度的基因定位。<br>r2和D’相比，受样本量和等位基因频率的影响较小（但影响仍然存在） </p>
<p><a href="https://img-blog.csdn.net/20161011171330149?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">label</a></p>
<p><strong>单体型(Haplotype,haploid genotype)</strong><br>是个体组织中，完全遗传自父母双方中一个亲本的一组等位基因，又称单倍体型或单元型。</p>
<p><strong>等位基因频率</strong><br>等位基因频率的定义如下：  </p>
<p>如果  </p>
<ol>
<li>一个染色体中存在某特定基因座，  </li>
<li>一个种群有N个个体，每一个个体的染色体套数为n（例如二倍体生物的体细胞中有两个该特定基因座），  </li>
<li>该等位基因在种群中有i份；</li>
</ol>
<p>那么等位基因频率为i&#x2F;(Nn)。</p>
<p>举例来说，如果在某种群中一个等位基因的基因频率为20%，那么在种群的所有成员中，1&#x2F;5的染色体带有那个等位基因，而其他4&#x2F;5的染色体带有该等位基因的其他对应变种——可以是一种也可以是很多种。</p>
<p>值得注意的是在二倍体基因中，带有该等位基因的个体最多可能有2&#x2F;5。如果等位基因随机分布的话，那么可以用二项式定理来计算：种群中32%的个体会是该等位基因的杂合体（带有一个该等位基因和另一个变种）, 4%的个体为该等位基因的纯合体（带有两个该等位基因）。所以加起来就有36%的个体带有该等位基因。然而，等位基因的随机分布是在选择不参与和其他前提下成立的。当这些前提成立时，一个种群的状态被称为哈蒂－温伯格平衡。  </p>
<p><strong>遗传连锁 genetic linkage：</strong><br>指有性生殖中的减数分裂期在同一染色体上两个位置靠得很近的基因的DNA序列有同时被遗传的倾向。若两个遗传标记所在的基因座相互靠得很近，则在染色体互换过程中它们被分离进入不同染色单体中的可能性较小，并因此得名为遗传“连锁”。<br>一般用<strong>厘摩（cM）</strong>单位来衡量遗传连锁的程度。两个遗传标记之间的距离为1 cM表示每100个减数分裂产物中，两者在1个中分离到了不同的染色体，或者说每50次减数分裂发生1次。</p>
<p><strong>区别连锁不平衡和遗传连锁：</strong><br>不同位点上不存在遗传连锁的等位基因之间可能存在连锁不平衡，且与等位基因频率是否处于平衡无关（不随时间变化）。此外，连锁不平衡有时被称为配子相不平衡；但是，该概念同样适用于无性生物，因此不依赖于配子的存在。</p>
<span id="jump">

<p><strong>哈迪-温伯格定律 Hardy-Weinberg principle</strong> </p>
</span> 

<p>一个族群在理想情况（不受特定的干扰因素影响，如非随机交配、天择、族群迁移、突变或群体大小有限），经过多个世代，基因频率与基因型频率会保持恒定并处于稳定的平衡状态。<br>最简单的例子是位于单一位点的两个等位基因：显性等位基因记为A而隐性等位基因记为$a$，它们的频率分别记为$p$和$q$. $P(A) &#x3D; p；P(a) &#x3D; q；p + q &#x3D; 1$。如果群体处于平衡状态，则我们可以得到  </p>
<p>$$P_{群体中纯合子AA}(AA) &#x3D; p2$$<br>$$P_{群体中纯合子aa}(aa) &#x3D; q2$$<br>$$P_{群体中杂合子Aa}(Aa) &#x3D; 2pq$$</p>
<p>自然界不太可能存在，哈温平衡常作遗传改变的基准。</p>
<p><strong>LD衰减</strong><br>LD的衰减指位点间由连锁不平衡到连锁平衡的演变过程；LD衰减的速度在不同物种间或同物种的不同亚群间，往往差异非常大。所以，通常会使用1个标准——“LD衰减距离”来描述LD衰减速度的快慢。<br>LD衰减距离通常指的是：当平均LD系数r2衰减到一定大小的时候，对应的物理距离。“一定大小”是这个定义的关键点，但没有特别统一的标准，在不同文章中标准不同。常见的标准包括：  </p>
<ul>
<li>LD系数降低到最大值的一半；</li>
<li>LD系数降低到0.5以下；</li>
<li>LD系数降低到0.1以下；</li>
<li>LD系数降低到基线水平（注意，不同物种的基线值是不同的）。</li>
</ul>
<p>值的获取：成对计算指定距离范围内的所有SNP的r2值，按区间取平均</p>
<p>在经典的群体遗传学范畴内，LD 的衰减是受重组率和重组代数(the number of generations of recombination) 影响的。因此，研究 LD 的衰减可以揭示群体重组的历史。比较慢的 LD 衰减模式一般认为与群体大小降低有关（Zhao et al, 2013）。与等位基因的频率相结合，LD 衰减也可以用于检测正向选择（Sebeti et al, 2002）。  </p>
<p>应用：</p>
<p>判断GWAS所需标记量，决定GWAS的检测效力以及精度<br>GWAS标记量 &#x3D; 基因组大小&#x2F;LD衰减距离<br>辅助分析进化与选择<br>在同一个连锁群上，LD衰减的慢说明该群体受到选择。一般来说，野生群体比驯化改良群体LD衰减快，异花授粉植物比自花授粉植物LD衰减快。比如玉米：地方品种1kb，自交系2kb，商用自交系100kb。  </p>
<p><strong>Haplotype Block</strong></p>
<p>单体型块，是生物体基因组中几乎没有遗传重组历史证据的一个区域，仅包含少数单倍型。此类区域应显示出高度连锁不平衡。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/13276792-d9957979b69ba8d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1047/format/webp" alt="Alt text"></p>
<span id="bukan">

<p><strong>gwas基本原理（以SNP分子标记为例）:</strong></p>
</span>

<p>1.在一定群体中选择病例组和对照组（对于数量性状则可以是连续分布的群体），比较全基因组范围内所有SNP位点的等位基因或者基因型频率在病例组和对照组间的差异，如果某个SNP位点的等位基因或基因型在病例组中出现的频率明显高于或低于对照组，则认为该位点与疾病间存在关联性.  </p>
<p>2.根据该位点在基因组中的位置和连锁不平衡关系推测可能的疾病易感基因. </p>
]]></content>
      <tags>
        <tag>内功</tag>
        <tag>气宗</tag>
      </tags>
  </entry>
  <entry>
    <title>Install Nodejs &amp; npm on CentOS</title>
    <url>/posts/1b1c601.html</url>
    <content><![CDATA[<span id="more"></span>
<p>要用jupyter插件离不开JavaScript。<br>那要用JS就得装Node.js和npm。<br>那装吧。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://npmmirror.com/mirrors/node/v18.16.0/node-v18.16.0-linux-x64.tar.xz</span><br><span class="line">tar -xvf node-v18.16.0-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>
<p>装在<code>root/Downloads/</code>目录下面。<br>然后cd到<code>usr/bin/</code>下，加个软链。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ln -s /root/Downloads/node-v18.16.0-linux-x64/bin/node node</span><br></pre></td></tr></table></figure>
<p>然后我们来node一下，哦豁，报错了。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@primary bin]# ./npm</span><br><span class="line">node: /lib64/libm.so.6: version `GLIBC_2.27&#x27; not found (required by node)</span><br></pre></td></tr></table></figure>
<p>得，GLIBC版本低，更新。参考<a href="https://www.cnblogs.com/dingshaohua/p/17103654.html">https://www.cnblogs.com/dingshaohua/p/17103654.html</a><br>丁少华dalao的解决方案</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://ftp.gnu.org/gnu/glibc/glibc-2.28.tar.gz</span><br><span class="line">tar xf glibc-2.28.tar.gz </span><br><span class="line">cd glibc-2.28/ &amp;&amp; mkdir build  &amp;&amp; cd build</span><br><span class="line">../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin</span><br></pre></td></tr></table></figure>
<p>报错，更新make和gcc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 升级GCC(默认为4 升级为8)</span><br><span class="line">yum install -y centos-release-scl</span><br><span class="line">yum install -y devtoolset-8-gcc*</span><br><span class="line">mv /usr/bin/gcc /usr/bin/gcc-4.8.5</span><br><span class="line">ln -s /opt/rh/devtoolset-8/root/bin/gcc /usr/bin/gcc</span><br><span class="line">mv /usr/bin/g++ /usr/bin/g++-4.8.5</span><br><span class="line">ln -s /opt/rh/devtoolset-8/root/bin/g++ /usr/bin/g++</span><br><span class="line"></span><br><span class="line"># 升级 make(默认为3 升级为4)</span><br><span class="line">wget http://ftp.gnu.org/gnu/make/make-4.3.tar.gz</span><br><span class="line">tar -xzvf make-4.3.tar.gz &amp;&amp; cd make-4.3/</span><br><span class="line">./configure  --prefix=/usr/local/make</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cd /usr/bin/ &amp;&amp; mv make make.bak</span><br><span class="line">ln -sv /usr/local/make/bin/make /usr/bin/make</span><br></pre></td></tr></table></figure>
<p>好，继续报错。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LD_LIBRARY_PATH shouldn&#x27;t contain the current directory</span><br></pre></td></tr></table></figure>
<p>问题不大，我们把这个变量删了。<br>先看一眼变量长啥样，copy下来以便恢复。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo $LD_LIBRARY_PATH # copy this path</span><br><span class="line">LD_LIBRARY_PATH=</span><br></pre></td></tr></table></figure>
<p>然后回过头去编译glibc。编译通过，<code>make &amp;&amp; make install</code>安装 重新添加LD_LIBRARY_PATH。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH[刚复制的路径]</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>有个要点，千万不要加空格。不然一会儿就<code>ls</code>都用不了了，系统会崩。会 段错误(吐核)。</p>
<p><strong>PS: 简单起见，<code>conda install nodejs</code>解决一切问题。（笑</strong></p>
]]></content>
      <tags>
        <tag>气宗</tag>
        <tag>工欲善其事</tag>
      </tags>
  </entry>
  <entry>
    <title>A personal site basing on github</title>
    <url>/posts/a59c950b.html</url>
    <content><![CDATA[<span id="more"></span>

<p>Welcome to my base. This is my first post. Hello world.<br>I have decided to post articles on my blog to pratice my English and document my life.<br>Because my English is poor, there are likely to be a lot of grammatical mistakes. But I believe that as time flies, I can improve my skills and express my ideas more freely.</p>
]]></content>
  </entry>
  <entry>
    <title>= =|||死都不输出的awk</title>
    <url>/posts/12b473ed.html</url>
    <content><![CDATA[<span id="more"></span>

<p>在用JupyterLab做查字典的时候，发现死都不输出，给ChatGPT验代码能通过，跑demo能通过。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">awk &#x27;NR==FNR&#123;a[$3]=$0;next&#125; NR&gt;FNR&#123;if($1 in a) print a[$1]&#125;&#x27; file1 file2 &gt; file3</span><br></pre></td></tr></table></figure>
<p>但就是死都不输出，挨个换输入文件发现问题出在字典里。<br>字典是直接从电脑通过JupyterLab拷贝过去的。可能出了和^M那个一样的莫名奇妙的问题。所以手动复制一个副本，愉快跑通。</p>
]]></content>
      <tags>
        <tag>气宗</tag>
      </tags>
  </entry>
  <entry>
    <title>深度神经网络名词解释</title>
    <url>/posts/85c7aca4.html</url>
    <content><![CDATA[<p><strong>反向传播</strong>（英语：Backpropagation，意为误差反向传播，缩写为BP）<br>是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变量计算损失函数的梯度，以更新权重来最小化损失函数。</p>
<p><strong>张量</strong>（英语：Tensor）是一个可用来表示在一些向量、标量和其他张量之间的线性关系的多线性函数，这些线性关系的基本例子有内积、外积、线性映射以及笛卡儿积。其坐标在$n$维空间内，有$n^r$个分量的一种量，其中每个分量都是坐标的函数，而在坐标变换时，这些分量也依照某些规则作线性变换。$r$称为该张量的秩或阶（与矩阵的秩和阶均无关系）。</p>
<p><strong>支持向量机</strong>（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。  </p>
<p>除了进行线性分类之外，SVM还可以使用所谓的核技巧有效地进行非线性分类，将其输入隐式映射到高维特征空间中。</p>
<p>当数据未被标记时，不能进行监督式学习，需要用非监督式学习，它会尝试找出数据到簇的自然聚类，并将新数据映射到这些已形成的簇。将支持向量机改进的聚类算法被称为支持向量聚类，当数据未被标记或者仅一些数据被标记时，支持向量聚类经常在工业应用中用作分类步骤的预处理。</p>
]]></content>
  </entry>
  <entry>
    <title>条件概率</title>
    <url>/posts/25dc542d.html</url>
    <content><![CDATA[<p>A、B为样本空间$\varOmega$中两事件，若P(B)&gt;0，则称：<br>$$P(A|B)&#x3D;\frac{P(AB)}{P(B)}$$<br>为“在B发生下A的条件概率”，简称条件概率。</p>
<p>乘法公式：<br>若$P(B)&gt;0$，则：<br>$$P(AB)&#x3D;P(B)P(A|B)$$</p>
]]></content>
      <tags>
        <tag>内力</tag>
      </tags>
  </entry>
  <entry>
    <title>配置JupyterLab</title>
    <url>/posts/c7d42074.html</url>
    <content><![CDATA[<span id="more"></span>

<p>嘛，要用服务器很麻烦的啦。<br>还得是JupyterLab。<br>我就在自己账户下配置了jupyterlab。轻松愉快。</p>
<ol>
<li>安装conda<br>自己百度，此处略。</li>
<li>进入conda环境，安装Jupyter Lab<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install -c conda-forge jupyterlab</span><br></pre></td></tr></table></figure>
之前用pip安装不是缺少这个包就是那个包，conda装挺好的。</li>
<li>配置JupyterLab<br>设置密码，当然不设置也可以（笑<br>后面会说token登录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from jupyter_server.auth import passwd</span><br><span class="line">passwd()</span><br></pre></td></tr></table></figure>
输入密码，然后verify，会生成一个加密的字符串。存一下。<br>之后生成配置文件。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jupyter lab --generate-config</span><br></pre></td></tr></table></figure>
会得到一个默认配置文件的位置，在隐藏文件夹<code>.jupyter/jupyter_lab_config.py</code>下面。打开它。里面有一万行注释。不管，直接加几行：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c.NotebookApp.ip = &quot;*&quot; # 允许所有ip访问</span><br><span class="line">c.NotebookApp.open_browser = False # 不打开浏览器</span><br><span class="line">c.NotebookApp.port = 8899 # 不用默认端口8888是美德</span><br><span class="line">c.NotebookApp.allow_remote_access = True # 允许远程访问</span><br><span class="line">c.NotebookApp.notebook_dir = &#x27;/home/username/jupyter_project&#x27; # 指定jupyter工作目录，我还没指定，据说不指定打开jupyter会404，但我没有404（笑</span><br></pre></td></tr></table></figure></li>
<li>启动jupyter<br>远端开一下<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup jupyter lab &amp;</span><br></pre></td></tr></table></figure>
在浏览器输入<code>[ip地址]:端口/lab</code><br>回车就进了。<br>然后需要输入密码。如果设置了密码那就输入设置的密码，没设置那就会要token<br>那token嘛就这么看<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jupyter lab list</span><br></pre></td></tr></table></figure>
会给一个网址，复制token填进去。</li>
</ol>
<p>关掉就kill掉jupyter进程就好了。</p>
]]></content>
      <tags>
        <tag>气宗</tag>
      </tags>
  </entry>
  <entry>
    <title>概率的性质</title>
    <url>/posts/506cea2e.html</url>
    <content><![CDATA[<ol>
<li>可加性、2. 连续性、3. 单调性<br><strong>加法公式</strong><br>对任意两个事件A、B，有：<br>$$P(A \cup B) &#x3D; P(A) + P(A) -P(AB)$$<br>一般地，对于任意n个事件$A_1,A_2,\dots,A_n,$有：<br>$$P\left( \bigcup^n_{i&#x3D;1}A_i\right) &#x3D; \sum_{i&#x3D;1}^nP(A_i)-\sum_{1\leq i &lt; j \le n }P(A_iA_j) + \sum_{1\leq i &lt; j  &lt; k \le n }P(A_iA_jA_k) + \dots + (-1)^{n-1}P(A_1A_2\dots A_n)$$<br>半可加性：<br>对任意两个事件，有：<br>$$P(A \cup B) \leq P(A) + P(B)$$</li>
</ol>
]]></content>
      <tags>
        <tag>内力</tag>
      </tags>
  </entry>
</search>
